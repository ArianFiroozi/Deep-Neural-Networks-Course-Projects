{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: ViT for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = Image.open(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append((filename, img))\n",
    "    return images\n",
    "\n",
    "directories = {\n",
    "    'neutrophil': './data/BCCD/neutrophil',\n",
    "    'eosinophil': './data/BCCD/eosinophil',\n",
    "    'monocyte': './data/BCCD/monocyte',\n",
    "    'lymphocyte': './data/BCCD/lymphocyte'\n",
    "}\n",
    "\n",
    "data = []\n",
    "for label, folder in directories.items():\n",
    "    imgs = load_images_from_folder(folder)\n",
    "    for img in imgs:\n",
    "        data.append((label, img[0], img[1]))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['label', 'filename', 'image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Histogram of Class Counts')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.core.bbox_utils import convert_bboxes_to_albumentations, convert_bboxes_from_albumentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = []\n",
    "\n",
    "for class_name, count in class_counts.items():\n",
    "    class_samples = df[df['label'] == class_name]\n",
    "    \n",
    "    while sum(1 if (x['label']==class_name) else 0 for x in augmented_data)<(250 - count):\n",
    "        sample = class_samples.sample(n=1).iloc[0]\n",
    "        image_path = os.path.join('./data/bccd', sample['label'],sample['filename'])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        augmented = transform(image=image)\n",
    "        augmented_image = augmented['image']\n",
    "\n",
    "        augmented_data.append({\n",
    "            'label': class_name,\n",
    "            'filename': f\"aug_{len(augmented_data)}_{sample['filename']}.jpg\",\n",
    "            'image': augmented_image,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "augmented_df = pd.concat([df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = augmented_df['label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.title('Histogram of Class Counts')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WBCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.df.iloc[idx]['image']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(augmented_df, test_size=0.2, random_state=42, stratify=augmented_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224))\n",
    "])\n",
    "\n",
    "train_dataset = WBCDataset(train_df, transform=transform)\n",
    "test_dataset = WBCDataset(test_df, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=4, ignore_mismatched_sizes=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=imgs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {label: idx for idx, label in enumerate(augmented_df['label'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  neutrophil       1.00      0.02      0.04        50\n",
      "  eosinophil       0.11      0.08      0.09        51\n",
      "    monocyte       0.00      0.00      0.00        50\n",
      "  lymphocyte       0.29      0.94      0.44        50\n",
      "\n",
      "    accuracy                           0.26       201\n",
      "   macro avg       0.35      0.26      0.14       201\n",
      "weighted avg       0.35      0.26      0.14       201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight is trainable.\n",
      "classifier.bias is trainable.\n"
     ]
    }
   ],
   "source": [
    "classifier_ft_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=4, ignore_mismatched_sizes=True)\n",
    "\n",
    "for name, param in classifier_ft_model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "for name, param in classifier_ft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3628612756729126\n",
      "Epoch 2, Loss: 1.2767224311828613\n",
      "Epoch 3, Loss: 1.2046312093734741\n",
      "Epoch 4, Loss: 1.1449741125106812\n",
      "Epoch 5, Loss: 1.0946063995361328\n",
      "Epoch 6, Loss: 1.0513287782669067\n",
      "Epoch 7, Loss: 1.0135787725448608\n",
      "Epoch 8, Loss: 0.9802276492118835\n",
      "Epoch 9, Loss: 0.9504493474960327\n",
      "Epoch 10, Loss: 0.923628032207489\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([{'params': classifier_ft_model.classifier.parameters(), 'lr': 1e-4},], lr=1e-4)\n",
    "\n",
    "classifier_ft_model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = classifier_ft_model(pixel_values=images, labels=torch.tensor([label_mapping[i] for i in labels]))\n",
    "        \n",
    "        loss = outputs.loss        \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier_ft_model(pixel_values=imgs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  neutrophil       0.78      0.84      0.81        50\n",
      "  eosinophil       0.67      0.63      0.65        51\n",
      "    monocyte       0.86      0.76      0.81        50\n",
      "  lymphocyte       0.80      0.88      0.84        50\n",
      "\n",
      "    accuracy                           0.78       201\n",
      "   macro avg       0.78      0.78      0.78       201\n",
      "weighted avg       0.78      0.78      0.77       201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85801732\n",
      "Trainable parameters: 3076\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in classifier_ft_model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classifier_ft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.encoder.layer.0.attention.attention.query.weight is trainable.\n",
      "vit.encoder.layer.0.attention.attention.query.bias is trainable.\n",
      "vit.encoder.layer.0.attention.attention.key.weight is trainable.\n",
      "vit.encoder.layer.0.attention.attention.key.bias is trainable.\n",
      "vit.encoder.layer.0.attention.attention.value.weight is trainable.\n",
      "vit.encoder.layer.0.attention.attention.value.bias is trainable.\n",
      "vit.encoder.layer.0.attention.output.dense.weight is trainable.\n",
      "vit.encoder.layer.0.attention.output.dense.bias is trainable.\n",
      "vit.encoder.layer.0.intermediate.dense.weight is trainable.\n",
      "vit.encoder.layer.0.intermediate.dense.bias is trainable.\n",
      "vit.encoder.layer.0.output.dense.weight is trainable.\n",
      "vit.encoder.layer.0.output.dense.bias is trainable.\n",
      "vit.encoder.layer.0.layernorm_before.weight is trainable.\n",
      "vit.encoder.layer.0.layernorm_before.bias is trainable.\n",
      "vit.encoder.layer.0.layernorm_after.weight is trainable.\n",
      "vit.encoder.layer.0.layernorm_after.bias is trainable.\n",
      "vit.encoder.layer.1.attention.attention.query.weight is trainable.\n",
      "vit.encoder.layer.1.attention.attention.query.bias is trainable.\n",
      "vit.encoder.layer.1.attention.attention.key.weight is trainable.\n",
      "vit.encoder.layer.1.attention.attention.key.bias is trainable.\n",
      "vit.encoder.layer.1.attention.attention.value.weight is trainable.\n",
      "vit.encoder.layer.1.attention.attention.value.bias is trainable.\n",
      "vit.encoder.layer.1.attention.output.dense.weight is trainable.\n",
      "vit.encoder.layer.1.attention.output.dense.bias is trainable.\n",
      "vit.encoder.layer.1.intermediate.dense.weight is trainable.\n",
      "vit.encoder.layer.1.intermediate.dense.bias is trainable.\n",
      "vit.encoder.layer.1.output.dense.weight is trainable.\n",
      "vit.encoder.layer.1.output.dense.bias is trainable.\n",
      "vit.encoder.layer.1.layernorm_before.weight is trainable.\n",
      "vit.encoder.layer.1.layernorm_before.bias is trainable.\n",
      "vit.encoder.layer.1.layernorm_after.weight is trainable.\n",
      "vit.encoder.layer.1.layernorm_after.bias is trainable.\n",
      "vit.encoder.layer.10.attention.attention.query.weight is trainable.\n",
      "vit.encoder.layer.10.attention.attention.query.bias is trainable.\n",
      "vit.encoder.layer.10.attention.attention.key.weight is trainable.\n",
      "vit.encoder.layer.10.attention.attention.key.bias is trainable.\n",
      "vit.encoder.layer.10.attention.attention.value.weight is trainable.\n",
      "vit.encoder.layer.10.attention.attention.value.bias is trainable.\n",
      "vit.encoder.layer.10.attention.output.dense.weight is trainable.\n",
      "vit.encoder.layer.10.attention.output.dense.bias is trainable.\n",
      "vit.encoder.layer.10.intermediate.dense.weight is trainable.\n",
      "vit.encoder.layer.10.intermediate.dense.bias is trainable.\n",
      "vit.encoder.layer.10.output.dense.weight is trainable.\n",
      "vit.encoder.layer.10.output.dense.bias is trainable.\n",
      "vit.encoder.layer.10.layernorm_before.weight is trainable.\n",
      "vit.encoder.layer.10.layernorm_before.bias is trainable.\n",
      "vit.encoder.layer.10.layernorm_after.weight is trainable.\n",
      "vit.encoder.layer.10.layernorm_after.bias is trainable.\n",
      "vit.encoder.layer.11.attention.attention.query.weight is trainable.\n",
      "vit.encoder.layer.11.attention.attention.query.bias is trainable.\n",
      "vit.encoder.layer.11.attention.attention.key.weight is trainable.\n",
      "vit.encoder.layer.11.attention.attention.key.bias is trainable.\n",
      "vit.encoder.layer.11.attention.attention.value.weight is trainable.\n",
      "vit.encoder.layer.11.attention.attention.value.bias is trainable.\n",
      "vit.encoder.layer.11.attention.output.dense.weight is trainable.\n",
      "vit.encoder.layer.11.attention.output.dense.bias is trainable.\n",
      "vit.encoder.layer.11.intermediate.dense.weight is trainable.\n",
      "vit.encoder.layer.11.intermediate.dense.bias is trainable.\n",
      "vit.encoder.layer.11.output.dense.weight is trainable.\n",
      "vit.encoder.layer.11.output.dense.bias is trainable.\n",
      "vit.encoder.layer.11.layernorm_before.weight is trainable.\n",
      "vit.encoder.layer.11.layernorm_before.bias is trainable.\n",
      "vit.encoder.layer.11.layernorm_after.weight is trainable.\n",
      "vit.encoder.layer.11.layernorm_after.bias is trainable.\n"
     ]
    }
   ],
   "source": [
    "first2_ft_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=4, ignore_mismatched_sizes=True)\n",
    "\n",
    "for name, param in first2_ft_model.named_parameters():\n",
    "    if (\"vit.encoder.layer.0\" not in name) and (\"vit.encoder.layer.1\" not in name):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "for name, param in first2_ft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85801732\n",
      "Trainable parameters: 28351488\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in first2_ft_model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in first2_ft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([{'params': first2_ft_model.classifier.parameters(), 'lr': 1e-4},], lr=1e-4)\n",
    "\n",
    "first2_ft_model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = first2_ft_model(pixel_values=images, labels=torch.tensor([label_mapping[i] for i in labels]))\n",
    "        \n",
    "        loss = outputs.loss        \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = first2_ft_model(pixel_values=imgs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### last 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.encoder.layer.10.attention.attention.query.weight is trainable.\n",
      "vit.encoder.layer.10.attention.attention.query.bias is trainable.\n",
      "vit.encoder.layer.10.attention.attention.key.weight is trainable.\n",
      "vit.encoder.layer.10.attention.attention.key.bias is trainable.\n",
      "vit.encoder.layer.10.attention.attention.value.weight is trainable.\n",
      "vit.encoder.layer.10.attention.attention.value.bias is trainable.\n",
      "vit.encoder.layer.10.attention.output.dense.weight is trainable.\n",
      "vit.encoder.layer.10.attention.output.dense.bias is trainable.\n",
      "vit.encoder.layer.10.intermediate.dense.weight is trainable.\n",
      "vit.encoder.layer.10.intermediate.dense.bias is trainable.\n",
      "vit.encoder.layer.10.output.dense.weight is trainable.\n",
      "vit.encoder.layer.10.output.dense.bias is trainable.\n",
      "vit.encoder.layer.10.layernorm_before.weight is trainable.\n",
      "vit.encoder.layer.10.layernorm_before.bias is trainable.\n",
      "vit.encoder.layer.10.layernorm_after.weight is trainable.\n",
      "vit.encoder.layer.10.layernorm_after.bias is trainable.\n",
      "vit.encoder.layer.11.attention.attention.query.weight is trainable.\n",
      "vit.encoder.layer.11.attention.attention.query.bias is trainable.\n",
      "vit.encoder.layer.11.attention.attention.key.weight is trainable.\n",
      "vit.encoder.layer.11.attention.attention.key.bias is trainable.\n",
      "vit.encoder.layer.11.attention.attention.value.weight is trainable.\n",
      "vit.encoder.layer.11.attention.attention.value.bias is trainable.\n",
      "vit.encoder.layer.11.attention.output.dense.weight is trainable.\n",
      "vit.encoder.layer.11.attention.output.dense.bias is trainable.\n",
      "vit.encoder.layer.11.intermediate.dense.weight is trainable.\n",
      "vit.encoder.layer.11.intermediate.dense.bias is trainable.\n",
      "vit.encoder.layer.11.output.dense.weight is trainable.\n",
      "vit.encoder.layer.11.output.dense.bias is trainable.\n",
      "vit.encoder.layer.11.layernorm_before.weight is trainable.\n",
      "vit.encoder.layer.11.layernorm_before.bias is trainable.\n",
      "vit.encoder.layer.11.layernorm_after.weight is trainable.\n",
      "vit.encoder.layer.11.layernorm_after.bias is trainable.\n"
     ]
    }
   ],
   "source": [
    "last2_ft_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=4, ignore_mismatched_sizes=True)\n",
    "\n",
    "for name, param in last2_ft_model.named_parameters():\n",
    "    if (\"vit.encoder.layer.10\" not in name) and (\"vit.encoder.layer.11\" not in name):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "for name, param in last2_ft_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85801732\n",
      "Trainable parameters: 14175744\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in last2_ft_model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in last2_ft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([{'params': last2_ft_model.classifier.parameters(), 'lr': 1e-4},], lr=1e-4)\n",
    "\n",
    "last2_ft_model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = last2_ft_model(pixel_values=images, labels=torch.tensor([label_mapping[i] for i in labels]))\n",
    "        \n",
    "        loss = outputs.loss        \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = last2_ft_model(pixel_values=imgs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "all_ft_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=4, ignore_mismatched_sizes=True)\n",
    "\n",
    "for name, param in all_ft_model.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85801732\n",
      "Trainable parameters: 85801732\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in all_ft_model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in all_ft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([{'params': all_ft_model.classifier.parameters(), 'lr': 1e-4},], lr=1e-4)\n",
    "\n",
    "all_ft_model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = all_ft_model(pixel_values=images, labels=torch.tensor([label_mapping[i] for i in labels]))\n",
    "        \n",
    "        loss = outputs.loss        \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = all_ft_model(pixel_values=imgs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "cnn_model = models.densenet121(pretrained=True)\n",
    "cnn_model.classifier = nn.Linear(cnn_model.classifier.in_features, 4)\n",
    "cnn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = cnn_model(imgs)\n",
    "        batch_predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  neutrophil       0.00      0.00      0.00        50\n",
      "  eosinophil       0.26      0.86      0.39        51\n",
      "    monocyte       0.27      0.12      0.17        50\n",
      "  lymphocyte       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.25       201\n",
      "   macro avg       0.13      0.25      0.14       201\n",
      "weighted avg       0.13      0.25      0.14       201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnn_model = models.densenet121(pretrained=True)\n",
    "all_cnn_model.classifier = nn.Linear(all_cnn_model.classifier.in_features, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 6957956\n",
      "Trainable parameters: 6957956\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in all_cnn_model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in all_cnn_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.371340274810791\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch        \n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mall_cnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, torch\u001b[38;5;241m.\u001b[39mtensor([label_mapping[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m labels]))\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()        \n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\densenet.py:213\u001b[0m, in \u001b[0;36mDenseNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 213\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(features, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    215\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(out, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\densenet.py:122\u001b[0m, in \u001b[0;36m_DenseBlock.forward\u001b[1;34m(self, init_features)\u001b[0m\n\u001b[0;32m    120\u001b[0m features \u001b[38;5;241m=\u001b[39m [init_features]\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 122\u001b[0m     new_features \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(new_features)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\densenet.py:88\u001b[0m, in \u001b[0;36m_DenseLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     86\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_checkpoint_bottleneck(prev_features)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(bottleneck_output)))\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\densenet.py:49\u001b[0m, in \u001b[0;36m_DenseLayer.bn_function\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbn_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     48\u001b[0m     concated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     bottleneck_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcated_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: T484\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([{'params': all_cnn_model.classifier.parameters(), 'lr': 1e-4},], lr=1e-4)\n",
    "\n",
    "all_cnn_model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = all_cnn_model(images)\n",
    "        loss = loss_fn(outputs, torch.tensor([label_mapping[i] for i in labels]))\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = all_cnn_model(imgs)\n",
    "        batch_predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  neutrophil       0.34      0.58      0.43        50\n",
      "  eosinophil       0.33      0.27      0.30        51\n",
      "    monocyte       0.39      0.34      0.36        50\n",
      "  lymphocyte       0.50      0.30      0.38        50\n",
      "\n",
      "    accuracy                           0.37       201\n",
      "   macro avg       0.39      0.37      0.37       201\n",
      "weighted avg       0.39      0.37      0.37       201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "classifier_cnn_model = models.densenet121(pretrained=True)\n",
    "classifier_cnn_model.classifier = nn.Linear(classifier_cnn_model.classifier.in_features, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight is trainable.\n",
      "classifier.bias is trainable.\n"
     ]
    }
   ],
   "source": [
    "for name, param in classifier_cnn_model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "for name, param in classifier_cnn_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name} is trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 6957956\n",
      "Trainable parameters: 4100\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in classifier_cnn_model.parameters())\n",
    "\n",
    "trainable_params = sum(p.numel() for p in classifier_cnn_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([{'params': classifier_cnn_model.classifier.parameters(), 'lr': 1e-4},], lr=1e-4)\n",
    "\n",
    "classifier_cnn_model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_data_loader:\n",
    "        images, labels = batch        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = classifier_cnn_model(images)\n",
    "        loss = loss_fn(outputs, torch.tensor([label_mapping[i] for i in labels]))\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for batch in test_data_loader:\n",
    "    imgs, labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier_cnn_model(imgs)\n",
    "        batch_predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    predictions.extend(batch_predictions)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  neutrophil       0.34      0.58      0.43        50\n",
      "  eosinophil       0.33      0.27      0.30        51\n",
      "    monocyte       0.39      0.34      0.36        50\n",
      "  lymphocyte       0.50      0.30      0.38        50\n",
      "\n",
      "    accuracy                           0.37       201\n",
      "   macro avg       0.39      0.37      0.37       201\n",
      "weighted avg       0.39      0.37      0.37       201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report([label_mapping[i] for i in ground_truth], predictions, target_names=label_mapping))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
